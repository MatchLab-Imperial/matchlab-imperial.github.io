---
---
@article{jing2025stereo,
	title={Stereo Any Video: Temporally Consistent Stereo Matching},
	url = {https://tomtomtommi.github.io/StereoAnyVideo/},
	journal={International Conference on Computer Vision},
	customauthor = {Junpeng Jing, Weixun Luo, Ye Mao, and Krystian Mikolajczyk},
	year={2025},
	volume={ICCV},
	image={/assets/images/publications/stereo_any_video.png},
	abstract={This paper introduces Stereo Any Video, a powerful framework for video stereo matching. It can estimate spatially accurate and temporally consistent disparities without relying on auxiliary information such as camera poses or optical flow. The strong capability is driven by rich priors from monocular video depth models, which are integrated with convolutional features to produce stable representations. To further enhance performance, key architectural innovations are introduced: all-to-all-pairs correlation, which constructs smooth and robust matching cost volumes, and temporal convex upsampling, which improves temporal coherence. These components collectively ensure robustness, accuracy, and temporal consistency, setting a new standard in video stereo matching. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple datasets both qualitatively and quantitatively in zero-shot settings, as well as strong generalization to real-world indoor and outdoor scenarios.},
}

@article{ye2025hypo,
	title = {Hypo3D: Exploring Hypothetical Reasoning in 3D},
	url = {https://matchlab-imperial.github.io/Hypo3D/},
	abstract = {The rise of vision-language foundation models marks an advancement in bridging the gap between human and machine capabilities in 3D scene reasoning. Existing 3D reasoning benchmarks assume real-time scene accessibility, which is impractical due to the high cost of frequent scene updates. To this end, we introduce Hypothetical 3D Reasoning, namely Hypo3D, a benchmark designed to evaluate models' ability to reason without access to real-time scene data. Models need to imagine the scene state based on a provided change description before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA) benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting in 14,885 question-answer pairs. An anchor-based world frame is established for all scenes, ensuring consistent reference to a global frame for directional terms in context changes and QAs. Extensive experiments show that state-of-the-art foundation models struggle to reason in hypothetically changed scenes. This reveals a substantial performance gap compared to humans, particularly in scenarios involving movement changes and directional reasoning. Even when the context change is irrelevant to the question, models often incorrectly adjust their answers.},
	journal = {International Conference on Machine Learning},
	customauthor = {Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, and Krystian Mikolajczyk},
	year = {2025},
	volume = {ICML},
    image = {/assets/images/publications/hypo_3d.png}
}

@article{ye2024open,
	title = {OpenDlign: Open-World Point Cloud Understanding with Depth-Aligned Images},
	url = {https://yebulabula.github.io/OpenDlign/},
	customauthor = {Ye Mao, Junpeng Jing, and Krystian Mikolajczyk},
	journal = {Advances in Neural Information Processing Systems},
	year = {2024},
	month = {september},
	volume = {NeurIPS},
	abstract = {Recent open-world 3D representation learning methods using Vision-Language Models (VLMs) to align 3D point cloud with image-text information have shown superior 3D zero-shot performance. However, CAD-rendered images for this alignment often lack realism and texture variation, compromising alignment robustness. Moreover, the volume discrepancy between 3D and 2D pretraining datasets highlights the need for effective strategies to transfer the representational abilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel open-world 3D model using depth-aligned images generated from a diffusion model for robust multimodal alignment. These images exhibit greater texture diversity than CAD renderings due to the stochastic nature of the diffusion model. By refining the depth map projection pipeline and designing depth-specific prompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D representation learning with streamlined fine-tuning. Our experiments show that OpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks, despite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In zero-shot classification, OpenDlign surpasses previous models by 8.0% on ModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images for multimodal alignment consistently enhances the performance of other state-of-the-art models.},
	image = {/assets/images/publications/open_dlign.png},
}

@article{jing2024bidavideo,
	title = {Match Stereo Videos via Bidirectional Alignment},
	url = {https://tomtomtommi.github.io/BiDAVideo/},
	customauthor = {Junpeng Jing, Ye Mao, Anlan Qiu, and Krystian Mikolajczyk},
	journal = {arXiv preprint},
	volume = {arXiv:2409.20283},
	year = {2024},
	image = {/assets/images/publications/bidavideo.png},
	abstract = {Video stereo matching is the task of estimating consistent disparity maps from rectified stereo videos. There is considerable scope for improvement in both datasets and methods within this area. Recent learning-based methods often focus on optimizing performance for independent stereo pairs, leading to temporal inconsistencies in videos. Existing video methods typically employ sliding window operation over time dimension, which can result in low-frequency oscillations corresponding to the window size. To address these challenges, we propose a bidirectional alignment mechanism for adjacent frames as a fundamental operation. Building on this, we introduce a novel video processing framework, BiDAStereo, and a plugin stabilizer network, BiDAStabilizer, compatible with general image-based methods. Regarding datasets, current synthetic object-based and indoor datasets are commonly used for training and benchmarking, with a lack of outdoor nature scenarios. To bridge this gap, we present a realistic synthetic dataset and benchmark focused on natural scenes, along with a real-world dataset captured by a stereo camera in diverse urban scenes for qualitative evaluation. Extensive experiments on in-domain, out-of-domain, and robustness evaluation demonstrate the contribution of our methods and datasets, showcasing improvements in prediction quality and achieving state-of-the-art results on various commonly used benchmarks.},
}

@article{jing2024bidastereo,
	title = {Match-Stereo-Videos: Bidirectional Alignment for Consistent Dynamic Stereo Matching},
	url = {https://tomtomtommi.github.io/BiDAStereo/},
	customauthor = {Junpeng Jing, Ye Mao, and Krystian Mikolajczyk},
	journal = {European Conference on Computer Vision},
	year = {2024},
	volume = {ECCV},
	image = {/assets/images/publications/bidastereo.png},
	abstract = {Dynamic stereo matching is the task of estimating consistent disparities from stereo videos with dynamic objects. Recent learning-based methods prioritize optimal performance on a single stereo pair, resulting in temporal inconsistencies. Existing video methods apply per-frame matching and window-based cost aggregation across the time dimension, leading to low-frequency oscillations at the scale of the window size. Towards this challenge, we develop a bidirectional alignment mechanism for adjacent frames as a fundamental operation. We further propose a novel framework, BiDAStereo, that achieves consistent dynamic stereo matching. Unlike the existing methods, we model this task as local matching and global aggregation. Locally, we consider correlation in a triple-frame manner to pool information from adjacent frames and improve the temporal consistency. Globally, to exploit the entire sequenceâ€™s consistency and extract dynamic scene cues for aggregation, we develop a motion-propagation recurrent unit. Extensive experiments demonstrate the performance of our method, showcasing improvements in prediction quality and achieving SoTA results on commonly used benchmarks.},
}

@article{roy2024understand,
	title = {Understanding the Role of the Projector in Knowledge Distillation},
	paper_url = {https://ojs.aaai.org/index.php/AAAI/article/view/28219},
	customauthor = {Roy Miles, and Krystian Mikolajczyk},
	journal = {AAAI Conference on Artificial Intelligence},
	year = {2024},
	volume = {AAAI},
	image = {/assets/images/publications/projector_in_kd.png},
	abstract = {In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are publicly available.},
}

@article{benedikt2024ucorr,
	title = {UCorr: Wire Detection and Depth Estimation for Autonomous Drones},
	paper_url = {https://link.springer.com/chapter/10.1007/978-3-031-59057-3_12},	
	customauthor = {Benedikt Kolbeinsson, and Krystian Mikolajczyk},
	journal = {International Conference on Robotics, Computer Vision and Intelligent Systems},
	year = {2024},
	volume = {ROBOVIS},
	image = {/assets/images/publications/ucorr.png},
	abstract = {In the realm of fully autonomous drones, the accurate detection of obstacles is paramount to ensure safe navigation and prevent collisions. Among these challenges, the detection of wires stands out due to their slender profile, which poses a unique and intricate problem. To address this issue, we present an innovative solution in the form of a monocular end-to-end model for wire segmentation and depth estimation. Our approach leverages a temporal correlation layer trained on synthetic data, providing the model with the ability to effectively tackle the complex joint task of wire detection and depth estimation. We demonstrate the superiority of our proposed method over existing competitive approaches in the joint task of wire detection and depth estimation. Our results underscore the potential of our model to enhance the safety and precision of autonomous drones, shedding light on its promising applications in real-world scenarios.},
}

@article{zhenyue2024aireyeseg,
	title = {AirEyeSeg: Teacher-Student Insights into Robust Fisheye UAV Detection},
	paper_url = {https://www.scitepress.org/Papers/2024/123886/123886.pdf},
	customauthor = {Zhenyue Gu, Benedikt Kolbeinsson, and Krystian Mikolajczyk},
	journal = {International Conference on Pattern Recognition Applications and Methods},
	year = {2024},
	volume = {ICPRAM},
	image = {/assets/images/publications/aireyeseg.png},
	abstract = {Accurate obstacle detection in Unmanned Aerial Vehicles (UAVs) using fisheye lenses is challenged by image distortions. While advanced algorithms like Fast Region-Based Convolutional Neural Network (Fast R-CNN), Spatial Pyramid Pooling-Net (SPP-Net), and You Only Look Once (YOLO) are proficient with standard images, they underperform on fisheye images due to serious distortions. We introduce a real-time fisheye object detection system for UAVs, underpinned by specialized fisheye datasets. Our contributions encompass the creation of UAV-centric fisheye datasets, a distillation-based (also termed Teacher-Student) training method, and AirEyeSeg, a pioneering fisheye detector. AirEyeSeg achieved a Mask(mAP50) of 88.6% for cars on the combined Visdorone and UAVid datasets and 84.5% for people on the SEE dataset using the Box(P) metric.  Our results demonstrate AirEyeSegâ€™s superiority over traditional detectors and validate our Teacher-Student training approach, setting a benchmark in fisheye-lensed UAV object detection.},
}

@article{benedikt2024ddos,
	title = {DDOS: the Drone Depth and Obstacle Segmentation Dataset},
	paper_url = {https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Kolbeinsson_DDOS_The_Drone_Depth_and_Obstacle_Segmentation_Dataset_CVPRW_2024_paper.html},
	customauthor = {Benedikt Kolbeinsson, and Krystian Mikolajczyk},
	journal = {IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
	year = {2024},
	volume = {CVPRW},
	image = {/assets/images/publications/ddos.png},
	abstract = {The advancement of autonomous drones, essential for sectors such as remote sensing and emergency services, is hindered by the absence of training datasets that fully capture the environmental challenges present in real-world scenarios, particularly operations in non-optimal weather conditions and the detection of thin structures like wires.  We present the Drone Depth and Obstacle Segmentation (DDOS) dataset to fill this critical gap with a collection of synthetic aerial images, created to provide comprehensive training samples for semantic segmentation and depth estimation. Specifically designed to enhance the identification of thin structures, DDOS allows drones to navigate a wide range of weather conditions, significantly elevating drone training and operational safety. Additionally, this work introduces innovative drone-specific metrics aimed at refining the evaluation of algorithms in depth estimation, with a focus on thin structure detection. These contributions not only pave the way for substantial improvements in autonomous drone technology but also set a new benchmark for future research, opening avenues for further advancements in drone navigation and safety.},
}

@article{benedikt2024multiclass,
	title = {Multi-class Segmentation from Aerial Views Using Recursive Noise Diffusion},
	paper_url = {https://openaccess.thecvf.com/content/WACV2024/html/Kolbeinsson_Multi-Class_Segmentation_From_Aerial_Views_Using_Recursive_Noise_Diffusion_WACV_2024_paper.html},
	customauthor = {Benedikt Kolbeinsson, and Krystian Mikolajczyk},
	journal = {IEEE/CVF Winter Conference on Applications of Computer Vision},
	year = {2024},
	volume = {WACV},
	image = {/assets/images/publications/multi_class.png},
	abstract = {Semantic segmentation from aerial views is a crucial task for autonomous drones, as they rely on precise and accurate segmentation to navigate safely and efficiently. However, aerial images present unique challenges such as diverse viewpoints, extreme scale variations, and high scene complexity. In this paper, we propose an end-to-end multi-class semantic segmentation diffusion model that addresses these challenges. We introduce recursive denoising to allow information to propagate through the denoising process, as well as a hierarchical multi-scale approach that complements the diffusion process. Our method achieves promising results on the UAVid dataset and state-of-the-art performance on the Vaihingen Building segmentation benchmark. Being the first iteration of this method, it shows great promise for future improvements.},
}

@article{adrian2023desc,
	title = {DESC: Domain Adaptation for Depth Estimation via Semantic Consistency},
	paper_url = {https://link.springer.com/article/10.1007/s11263-022-01718-1},
	customauthor = {Adrian Lopez-Rodriguez, and Krystian Mikolajczyk},
	journal = {International Journal of Computer Vision},
	year = {2023},
	volume = {IJCV},
	image = {/assets/images/publications/desc.png},
	abstract = {Accurate real depth annotations are difficult to acquire, needing the use of special devices such as a LiDAR sensor. Self-supervised methods try to overcome this problem by processing video or stereo sequences, which may not always be available. Instead, in this paper, we propose a domain adaptation approach to train a monocular depth estimation model using a fully-annotated source dataset and a non-annotated target dataset. We bridge the domain gap by leveraging semantic predictions and low-level edge features to provide guidance for the target domain. We enforce consistency between the main model and a second model trained with semantic segmentation and edge maps, and introduce priors in the form of instance heights. Our approach is evaluated on standard domain adaptation benchmarks for monocular depth estimation and show consistent improvement upon the state-of-the-art.},
}

@article{dylan2023learning,
	title = {Learning to Prompt CLIP for Monocular Depth Estimation: Exploring the Limits of Human Language},
	paper_url = {https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Auty_Learning_to_Prompt_CLIP_for_Monocular_Depth_Estimation_Exploring_the_ICCVW_2023_paper.html},
	customauthor = {Dylan Auty, and Krystian Mikolajczyk},
	journal = {IEEE/CVF International Conference on Computer Vision Workshops},
	year = {2023},
	volume = {ICCVW},
	image = {/assets/images/publications/learning_to_prompt_clip.png},
	abstract = {CLIP is a significant vision-and-language training framework that has shown surprisingly general understanding of the world, with good performance in many open-ended tasks with little or no additional training. A recent technique has used CLIP to perform 0-shot Monocular Depth Estimation (MDE) by using depth-related prompts, but the use of human language in these prompts presents an unnecessary human bias. In this work, we use continuous learnable tokens in place of discrete human-language words to shed light on the problem. We achieve a significant boost in performance, and find that the learned tokens do not map neatly to depth-related human language, implying that CLIP's concept of depth is not succinctly expressible in human language. We posit that this may extend to other CLIP concepts, and believe that this finding will spark further research into both the use and interpretation of non-linguistic tokens in all open-ended scene interpretation tasks.},
}

@article{roy2023reconstruct,
	title = {Reconstructing Pruned Filters Using Cheap Spatial Transformations},
	paper_url = {https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Miles_Reconstructing_Pruned_Filters_Using_Cheap_Spatial_Transformations_ICCVW_2023_paper.html},
	customauthor = {Roy Miles, and Krystian Mikolajczyk},
	journal = {IEEE/CVF International Conference on Computer Vision Workshops},
	year = {2023},
	volume = {ICCVW},
	image = {/assets/images/publications/reconstruct.png},
	abstract = {We present an efficient alternative to the convolutional layer using cheap spatial transformations. This construction exploits an inherent spatial redundancy of the learned convolutional filters to enable a much greater parameter efficiency, while maintaining the top-end accuracy of their dense counter-parts. Training these networks is modelled as a generalised pruning problem, whereby the pruned filters are replaced with cheap transformations from the set of non-pruned filters. We provide an efficient implementation of the proposed layer, followed by two natural extensions to avoid excessive feature compression and to improve the expressivity of the transformed features. We show that these networks can achieve comparable or improved performance to state-of-the-art pruning models across both the CIFAR-10 and ImageNet-1K datasets.},
}